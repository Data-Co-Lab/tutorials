{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification \n",
    "###  [Challenge on Kaggle](https://www.kaggle.com/c/gdg-manouba-challenge/leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [cv2.imread(file) for file in glob.glob(\"train/*.png\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Plot of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3d9a66d080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images classes are in the pictures names, these classes are : Dog, Cat, Bird, Hourse, Deer, Frog, Truck, Ship,Airplane,Automobile,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"train/\"):\n",
    "    f.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24346_horse.png',\n",
       " '3777_ship.png',\n",
       " '34844_horse.png',\n",
       " '20247_horse.png',\n",
       " '24947_dog.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will remove the number and .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for name in f:\n",
    "    name=name[name.find('_')+1:]\n",
    "    name=name.replace(\".png\",\"\")\n",
    "    names.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horse', 'ship', 'horse', 'horse', 'dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A colored image is presented with 3 matrices, for Red, Green and Blue. The matrix dimension is the same as the image dimension in pixels. each pixel can take a value from 0 to 255. We will devide each value in every matrix by 255 to normalize its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for image in images:\n",
    "    image=image/255\n",
    "    train.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before training the model, first we will use a technique called 'One-hot Encoding'. In short, we will be presenting each class with a vector with a single value set to '1' while others are set to '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse' 'ship' 'horse' ... 'deer' 'bird' 'bird']\n"
     ]
    }
   ],
   "source": [
    "# define example\n",
    "values = array(names)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 8 7 ... 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(names)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list to an array\n",
    "onehot_encoded = np.array(onehot_encoded[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=Train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as keras\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,MaxPooling2D,Dropout,Flatten\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use a 2D convolution neural network model, composed of : \n",
    "1. __Conv2D:__ 2D convolution layer\n",
    "2. __MaxPooling2D:__ Check [Here](https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks) for more explanation \n",
    "3. __Dropout:__ randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "4. __Flatten:__ transform a two-dimensional matrix of features into a vector\n",
    "5. __Dense:__ it is used to change the dimensions of your vector (apply a Rotation), we use __Softmax__ in the last layer to map the non-normalized output of our network to a probability distribution over predicted output classes.\n",
    "#### You can check [Keras documentation](https://keras.io) for more details \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/badri/.conda/envs/Vynd/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),  padding='same', activation='relu', input_shape=(32, 32,3))) \n",
    "#Image representation : 32px*32px for 3 matrices    \n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))# (16,16,32)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),   padding='same', activation='relu'))   \n",
    "model.add(Conv2D(64, (3,3),  padding='same', activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))# (8,8,64)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),   padding='same', activation='relu'))   \n",
    "model.add(Conv2D(64, (3,3),  padding='same', activation='relu')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))# (4,4,128)\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))   # Final Layer using Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and check the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,354\n",
      "Trainable params: 669,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We train the model with 80% of our data, and use the 20% remaining for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/badri/.conda/envs/Vynd/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 96s 2ms/step - loss: 1.7147 - accuracy: 0.3646 - val_loss: 1.2973 - val_accuracy: 0.5279\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 1.2802 - accuracy: 0.5361 - val_loss: 1.0918 - val_accuracy: 0.6068\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 71s 2ms/step - loss: 1.1182 - accuracy: 0.5986 - val_loss: 1.0105 - val_accuracy: 0.6356\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 1.0149 - accuracy: 0.6376 - val_loss: 0.8779 - val_accuracy: 0.6839\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.9466 - accuracy: 0.6647 - val_loss: 0.8414 - val_accuracy: 0.7059\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 68s 2ms/step - loss: 0.8931 - accuracy: 0.6831 - val_loss: 0.8640 - val_accuracy: 0.6963\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 0.8655 - accuracy: 0.6961 - val_loss: 0.7768 - val_accuracy: 0.7231\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 0.8244 - accuracy: 0.7093 - val_loss: 0.7988 - val_accuracy: 0.7238\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 0.8080 - accuracy: 0.7182 - val_loss: 0.7139 - val_accuracy: 0.7526\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 56s 1ms/step - loss: 0.7896 - accuracy: 0.7235 - val_loss: 0.7061 - val_accuracy: 0.7570\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 57s 1ms/step - loss: 0.7672 - accuracy: 0.7301 - val_loss: 0.6942 - val_accuracy: 0.7537\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 0.7538 - accuracy: 0.7348 - val_loss: 0.7095 - val_accuracy: 0.7534\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 93s 2ms/step - loss: 0.7354 - accuracy: 0.7394 - val_loss: 0.6873 - val_accuracy: 0.7575\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 80s 2ms/step - loss: 0.7312 - accuracy: 0.7426 - val_loss: 0.7233 - val_accuracy: 0.7501\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.7166 - accuracy: 0.7456 - val_loss: 0.6603 - val_accuracy: 0.7679\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 73s 2ms/step - loss: 0.7085 - accuracy: 0.7513 - val_loss: 0.6895 - val_accuracy: 0.7640\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.6951 - accuracy: 0.7556 - val_loss: 0.6865 - val_accuracy: 0.7656\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.6950 - accuracy: 0.7577 - val_loss: 0.6989 - val_accuracy: 0.7566\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6893 - accuracy: 0.7590 - val_loss: 0.6590 - val_accuracy: 0.7755\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6835 - accuracy: 0.7614 - val_loss: 0.7011 - val_accuracy: 0.7567\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6829 - accuracy: 0.7608 - val_loss: 0.6686 - val_accuracy: 0.7692\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6741 - accuracy: 0.7651 - val_loss: 0.6369 - val_accuracy: 0.7798\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6647 - accuracy: 0.7656 - val_loss: 0.6441 - val_accuracy: 0.7788\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6658 - accuracy: 0.7683 - val_loss: 0.6943 - val_accuracy: 0.7672\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6593 - accuracy: 0.7701 - val_loss: 0.6620 - val_accuracy: 0.7742\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6529 - accuracy: 0.7709 - val_loss: 0.6958 - val_accuracy: 0.7662\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 0.6488 - accuracy: 0.7737 - val_loss: 0.6629 - val_accuracy: 0.7787\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6415 - accuracy: 0.7736 - val_loss: 0.6391 - val_accuracy: 0.7831\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6549 - accuracy: 0.7713 - val_loss: 0.6523 - val_accuracy: 0.7767\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6417 - accuracy: 0.7740 - val_loss: 0.7084 - val_accuracy: 0.7585\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6363 - accuracy: 0.7764 - val_loss: 0.6470 - val_accuracy: 0.7786\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6340 - accuracy: 0.7774 - val_loss: 0.6280 - val_accuracy: 0.7852\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6324 - accuracy: 0.7784 - val_loss: 0.6260 - val_accuracy: 0.7929\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 70s 2ms/step - loss: 0.6249 - accuracy: 0.7803 - val_loss: 0.6929 - val_accuracy: 0.7737\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6252 - accuracy: 0.7823 - val_loss: 0.6424 - val_accuracy: 0.7886\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 61s 2ms/step - loss: 0.6277 - accuracy: 0.7799 - val_loss: 0.6417 - val_accuracy: 0.7758\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 61s 2ms/step - loss: 0.6295 - accuracy: 0.7796 - val_loss: 0.6391 - val_accuracy: 0.7783\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6232 - accuracy: 0.7832 - val_loss: 0.6524 - val_accuracy: 0.7802\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6173 - accuracy: 0.7842 - val_loss: 0.6516 - val_accuracy: 0.7804\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6285 - accuracy: 0.7832 - val_loss: 0.6418 - val_accuracy: 0.7844\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6189 - accuracy: 0.7829 - val_loss: 0.6207 - val_accuracy: 0.7909\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6179 - accuracy: 0.7857 - val_loss: 0.6560 - val_accuracy: 0.7806\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6087 - accuracy: 0.7873 - val_loss: 0.6365 - val_accuracy: 0.7857\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6080 - accuracy: 0.7877 - val_loss: 0.6412 - val_accuracy: 0.7797\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6154 - accuracy: 0.7846 - val_loss: 0.6371 - val_accuracy: 0.7882\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.6104 - accuracy: 0.7869 - val_loss: 0.6336 - val_accuracy: 0.7888\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.6098 - accuracy: 0.7903 - val_loss: 0.6548 - val_accuracy: 0.7803\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.5963 - accuracy: 0.7925 - val_loss: 0.6499 - val_accuracy: 0.7811\n",
      "Epoch 49/50\n",
      "40000/40000 [==============================] - 91s 2ms/step - loss: 0.6030 - accuracy: 0.7885 - val_loss: 0.6676 - val_accuracy: 0.7794\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.6085 - accuracy: 0.7880 - val_loss: 0.6373 - val_accuracy: 0.7873\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(Train,onehot_encoded,batch_size=32,epochs=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've reached an accurency of 78% based on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We save the model future user or further improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we load the test data, and predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=[]\n",
    "for i in range (10000):\n",
    "    path = \"test/\"+ str(i) + \".png\"\n",
    "    img = cv2.imread(path)/255\n",
    "    Test.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=np.array(Test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict_classes(Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we reverse the encoding to restore each class name as demanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred=list(label_encoder.inverse_transform(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's write the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(Preds)):\n",
    "    Sub.at[i,'label']=Preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub.index=Sub.index.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     label\n",
       "0      0       cat\n",
       "1      1      ship\n",
       "2      2      ship\n",
       "3      3  airplane\n",
       "4      4      frog"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub.to_csv('Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our submission got 78.32%, wich is identical to our training results\n",
    "![alt text](Submission.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model can be further improved, by adding more layers, adjusting some parameters, so feel free to try it yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}